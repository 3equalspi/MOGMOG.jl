{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01404366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/.julia/dev/MOGMOG.jl`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36312a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MOGMOG\n",
    "using DLProteinFormats\n",
    "using Flux\n",
    "using Onion\n",
    "using RandomFeatureMaps\n",
    "using Onion.Einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7821ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, JLD2, Random\n",
    "\n",
    "struct Molecule\n",
    "    atoms::Vector{String}\n",
    "    positions::Matrix{Float64}\n",
    "end\n",
    "\n",
    "Base.length(mol::Molecule) = length(mol.atoms)\n",
    "\n",
    "# Load processed molecules\n",
    "# @load expanduser(\"~/processed_molecules.jld2\") result  # Loads `result::Vector{Molecule}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae9628e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "atom_dict = Dict(name => i for (i, name) in enumerate([\"C\", \"F\", \"H\", \"N\", \"O\", \"STOP\"]))\n",
    "PAD = atom_dict[\"STOP\"]\n",
    "vocab_size = length(atom_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72416c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(foot = (current_coord_embed = (layers = ((W = (),), (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),), atom_embed = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m,), position_embed = (layers = ((W = (),), (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),), next_x_embed = (layers = ((W = (),), (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),), next_y_embed = (layers = ((W = (),), (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),)), body = @NamedTuple{transformer::@NamedTuple{attention::@NamedTuple{wq::@NamedTuple{weight::Optimisers.Leaf{AdamW{Float32, Tuple{Float64, Float64}, Float64, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, wk::@NamedTuple{weight::Optimisers.Leaf{AdamW{Float32, Tuple{Float64, Float64}, Float64, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, wv::@NamedTuple{weight::Optimisers.Leaf{AdamW{Float32, Tuple{Float64, Float64}, Float64, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, wo::@NamedTuple{weight::Optimisers.Leaf{AdamW{Float32, Tuple{Float64, Float64}, Float64, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, dim::Tuple{}, n_heads::Tuple{}, n_kv_heads::Tuple{}, head_dim::Tuple{}}, feed_forward::@NamedTuple{w1::@NamedTuple{weight::Optimisers.Leaf{AdamW{Float32, Tuple{Float64, Float64}, Float64, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, w2::@NamedTuple{weight::Optimisers.Leaf{AdamW{Float32, Tuple{Float64, Float64}, Float64, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, w3::@NamedTuple{weight::Optimisers.Leaf{AdamW{Float32, Tuple{Float64, Float64}, Float64, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, act::Tuple{}}, attention_norm::@NamedTuple{weight::Optimisers.Leaf{AdamW{Float32, Tuple{Float64, Float64}, Float64, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, eps::Tuple{}}, ffn_norm::@NamedTuple{weight::Optimisers.Leaf{AdamW{Float32, Tuple{Float64, Float64}, Float64, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, eps::Tuple{}}}}[(transformer = (attention = (wq = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), feed_forward = (w1 = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), w2 = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), w3 = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), act = ()), attention_norm = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), ffn_norm = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ())),), (transformer = (attention = (wq = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), feed_forward = (w1 = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), w2 = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), w3 = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), act = ()), attention_norm = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), ffn_norm = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ())),), (transformer = (attention = (wq = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), feed_forward = (w1 = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), w2 = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), w3 = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), act = ()), attention_norm = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), ffn_norm = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ())),), (transformer = (attention = (wq = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), feed_forward = (w1 = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), w2 = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), w3 = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), act = ()), attention_norm = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), ffn_norm = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ())),)], mog_head = (linear_μ = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), linear_σ = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), linear_logw = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())), atom_head = (linear_logits = (weight = \u001b[32mLeaf(AdamW(eta=0.001, beta=(0.9, 0.999), lambda=0.0, epsilon=1.0e-8, couple=true), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Model\n",
    "embed_dim = 128\n",
    "n_components = 5\n",
    "vocab_size = length(atom_dict)\n",
    "model = MOGMOGModel(embed_dim, n_components, vocab_size, depth=4)\n",
    "\n",
    "# Optimizer\n",
    "opt_state = Flux.setup(AdamW(0.001f0), model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a0e4abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1119119"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(length.(Flux.trainables(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf5f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pad_batch (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Pad function\n",
    "function pad_batch(mols::Vector{Molecule})\n",
    "    max_len = maximum(length.(mols)) + 1  # Find the maximum length of molecules\n",
    "    B = length(mols)\n",
    "    atom_types = fill(PAD, max_len, B)\n",
    "    coordinates = zeros(Float32, 3, max_len, B) .+ 0\n",
    "    atom_type_mask = zeros(Float32, max_len - 1, B)\n",
    "    coordinate_mask = zeros(Float32, max_len - 1, B)\n",
    "\n",
    "    for (i, mol) in enumerate(mols)\n",
    "        L = length(mol)\n",
    "        for j in 1:L\n",
    "            atom_types[j, i] = get(atom_dict, mol.atoms[j], PAD)\n",
    "        end\n",
    "        coordinates[:, 1:L, i] = mol.positions[1:L, :]'\n",
    "        coordinate_mask[1:L-1, i] .= 1.0\n",
    "        atom_type_mask[1:L, i] .= 1.0\n",
    "    end\n",
    "    return atom_types, coordinates, atom_type_mask, coordinate_mask\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c76d701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×2 Matrix{Int64}:\n",
       " 1  1\n",
       " 3  2\n",
       " 3  1\n",
       " 3  6\n",
       " 3  6\n",
       " 6  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3×6×2 Array{Float32, 3}:\n",
       "[:, :, 1] =\n",
       "  0.158273   1.62107   1.8172    1.27361   0.828848  0.0\n",
       "  1.18803   -1.0081    0.363998  1.05924  -0.846691  0.0\n",
       " -0.484876  -1.13739  -1.07275   1.26865   1.18528   0.0\n",
       "\n",
       "[:, :, 2] =\n",
       " -1.48635   -0.443743  1.24576   0.0  0.0  0.0\n",
       " -0.355486  -0.140105  0.325522  0.0  0.0  0.0\n",
       " -0.925501  -0.544627  0.782694  0.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×2 Matrix{Float32}:\n",
       " 1.0  1.0\n",
       " 1.0  1.0\n",
       " 1.0  1.0\n",
       " 1.0  0.0\n",
       " 1.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×2 Matrix{Float32}:\n",
       " 1.0  1.0\n",
       " 1.0  1.0\n",
       " 1.0  0.0\n",
       " 1.0  0.0\n",
       " 0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = [\n",
    "    Molecule([\"C\", \"H\", \"H\", \"H\", \"H\"], randn(5, 3)),\n",
    "    Molecule([\"C\", \"F\", \"C\"], randn(3, 3)),\n",
    "]\n",
    "\n",
    "pad_batch(result) .|> display;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7af4fafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coordinates = zeros(Float32, 3, max_len, B) .+ 0 = Float32[0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0;;; 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.972348943862457"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss(model, pad_batch(result)...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0ddc697b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coordinates = zeros(Float32, 3, max_len, B) .+ 0 = Float32[0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0;;; 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.86637760749771"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss(model, pad_batch(result)...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad9dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 1\n",
    "nbatches = 100\n",
    "batchsize = 16\n",
    "max_len = 32\n",
    "all_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in 1:nepochs\n",
    "    @info \"Epoch $epoch\"\n",
    "    shuffled = shuffle(result)\n",
    "    epoch_losses = []\n",
    "\n",
    "    for i in 1:nbatches\n",
    "        mols = shuffled[i:i+batchsize-1]\n",
    "        atom_ids, positions, atom_mask, coordinate_mask = pad_batch(mols, atom_dict, max_len)\n",
    "\n",
    "        loss_batch, (grad_model,) = Flux.withgradient(model) do m\n",
    "            loss(m, atom_ids, positions, atom_mask, coordinate_mask)\n",
    "        end\n",
    "\n",
    "        Flux.update!(opt_state, model, grad_model)\n",
    "        push!(all_losses, loss_batch)\n",
    "        push!(epoch_losses, loss_batch)\n",
    "\n",
    "        println(\"Batch $i, loss = $(round(loss_batch, digits=4))\")\n",
    "    end\n",
    "\n",
    "    println(\"Epoch $epoch done. Avg loss = $(round(mean(epoch_losses), digits=4))\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5449f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DLProteinFormats.load(PDBSimpleFlat500)\n",
    "\n",
    "locations = [d.locs for d in data]\n",
    "sequences = [d.AAs for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afa6494",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec(locations[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "82f23a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{DART{TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}}}:\n",
       " DART{TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}}(TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), 64, 1, 1, 64), StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}(Dense(64 => 256; bias=false), Dense(256 => 64; bias=false), Dense(64 => 256; bias=false), NNlib.swish), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5)))  \u001b[90m# 65_664 parameters\u001b[39m\n",
       " DART{TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}}(TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), 64, 1, 1, 64), StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}(Dense(64 => 256; bias=false), Dense(256 => 64; bias=false), Dense(64 => 256; bias=false), NNlib.swish), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5)))  \u001b[90m# 65_664 parameters\u001b[39m\n",
       " DART{TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}}(TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), 64, 1, 1, 64), StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}(Dense(64 => 256; bias=false), Dense(256 => 64; bias=false), Dense(64 => 256; bias=false), NNlib.swish), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5)))  \u001b[90m# 65_664 parameters\u001b[39m\n",
       " DART{TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}}(TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), 64, 1, 1, 64), StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}(Dense(64 => 256; bias=false), Dense(256 => 64; bias=false), Dense(64 => 256; bias=false), NNlib.swish), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5)))  \u001b[90m# 65_664 parameters\u001b[39m\n",
       " DART{TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}}(TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), 64, 1, 1, 64), StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}(Dense(64 => 256; bias=false), Dense(256 => 64; bias=false), Dense(64 => 256; bias=false), NNlib.swish), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5)))  \u001b[90m# 65_664 parameters\u001b[39m\n",
       " DART{TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}}(TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), 64, 1, 1, 64), StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}(Dense(64 => 256; bias=false), Dense(256 => 64; bias=false), Dense(64 => 256; bias=false), NNlib.swish), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5)))  \u001b[90m# 65_664 parameters\u001b[39m\n",
       " DART{TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}}(TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), 64, 1, 1, 64), StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}(Dense(64 => 256; bias=false), Dense(256 => 64; bias=false), Dense(64 => 256; bias=false), NNlib.swish), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5)))  \u001b[90m# 65_664 parameters\u001b[39m\n",
       " DART{TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}}(TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), 64, 1, 1, 64), StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}(Dense(64 => 256; bias=false), Dense(256 => 64; bias=false), Dense(64 => 256; bias=false), NNlib.swish), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5)))  \u001b[90m# 65_664 parameters\u001b[39m\n",
       " DART{TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}}(TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), 64, 1, 1, 64), StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}(Dense(64 => 256; bias=false), Dense(256 => 64; bias=false), Dense(64 => 256; bias=false), NNlib.swish), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5)))  \u001b[90m# 65_664 parameters\u001b[39m\n",
       " DART{TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}}(TransformerBlock{Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}, StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}, RMSNorm{Float32, Vector{Float32}}, RMSNorm{Float32, Vector{Float32}}}(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), Dense(64 => 64; bias=false), 64, 1, 1, 64), StarGLU{Dense{typeof(identity), Matrix{Float32}, Bool}, typeof(swish)}(Dense(64 => 256; bias=false), Dense(256 => 64; bias=false), Dense(64 => 256; bias=false), NNlib.swish), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0f-5)))  \u001b[90m# 65_664 parameters\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rff_dim = 32\n",
    "embedding_dim = 64\n",
    "\n",
    "rff = RandomFourierFeatures(1 => rff_dim, 0.1f0)\n",
    "pre_transformer_proj = Dense(rff_dim => embedding_dim, bias=false)\n",
    "transformer_blocks = [DART(TransformerBlock(64, 1, 1)) for i in 1:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "50537613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×3×284 Array{Float32, 3}:\n",
       "[:, :, 1] =\n",
       " -2407.14   -2562.78     -937.067\n",
       "  1286.41    1602.41     1029.42\n",
       " -1107.37    -540.147   -1204.58\n",
       "  -736.283   -315.754    -683.237\n",
       " -2334.51   -1678.19    -1276.43\n",
       "  3111.12    3049.13     1381.32\n",
       "  -273.528   -723.555    -542.492\n",
       "  -748.07    -811.119    -315.506\n",
       "   637.612    -45.3017   -680.749\n",
       "   628.865    433.497    -279.523\n",
       "     ⋮                  \n",
       "  -441.058    626.187   -1134.05\n",
       "   702.076    842.322    1670.47\n",
       "  2093.78    1814.72     1226.65\n",
       " -2099.05   -2215.23    -2761.29\n",
       "  -899.831   -763.283    -875.541\n",
       "  -339.231   -159.238    -372.918\n",
       " -2202.51   -2224.38     -972.282\n",
       "  -278.958    320.072     268.213\n",
       " -1407.58   -1092.7      -549.213\n",
       "\n",
       "[:, :, 2] =\n",
       " -1920.8    -2014.27     -925.375\n",
       "  1322.87    1584.71      970.839\n",
       "  -600.665   -331.768   -1212.94\n",
       "  -211.425    -80.7043   -705.439\n",
       " -1695.49   -1455.84    -1326.92\n",
       "  2645.61    2690.94     1319.66\n",
       "  -398.563   -688.619    -630.396\n",
       "  -737.094   -720.233    -265.68\n",
       "   486.908     42.0931   -784.838\n",
       "   774.924    620.411    -274.668\n",
       "     ⋮                  \n",
       "  -144.166    545.851   -1162.69\n",
       "   596.592    761.036    1747.21\n",
       "  1700.27    1654.89     1222.64\n",
       " -2071.01   -2281.71    -2829.21\n",
       "  -895.914   -855.279    -910.808\n",
       "  -314.714   -192.519    -325.885\n",
       " -2224.49   -2215.56     -985.934\n",
       "  -284.607    196.224     190.93\n",
       " -1109.53    -920.408    -461.087\n",
       "\n",
       "[:, :, 3] =\n",
       " -1845.74    -1946.54     -968.642\n",
       "  1340.76     1474.86      891.324\n",
       "  -470.406    -356.239   -1292.11\n",
       "  -125.736     -62.0216   -833.15\n",
       " -1594.46    -1502.13    -1513.23\n",
       "  2544.51     2596.92     1366.08\n",
       "  -456.788    -579.082    -675.359\n",
       "  -719.281    -742.276    -205.033\n",
       "   417.438     250.312    -786.806\n",
       "   799.054     756.982    -205.547\n",
       "     ⋮                   \n",
       "   -59.5278    260.159   -1177.61\n",
       "   614.905     685.336    1782.61\n",
       "  1649.86     1642.31     1281.46\n",
       " -2088.88    -2200.88    -2935.89\n",
       "  -895.384    -877.424    -975.702\n",
       "  -290.984    -233.959    -270.163\n",
       " -2242.73    -2256.3      -996.807\n",
       "  -271.804     -37.8607     58.4337\n",
       " -1045.76     -979.75     -401.268\n",
       "\n",
       ";;; … \n",
       "\n",
       "[:, :, 282] =\n",
       " -1892.36    -1576.69    -1389.83\n",
       "  1775.87     1817.76      758.69\n",
       "   -69.3121    -29.3352  -1294.41\n",
       "  -268.2      -456.91    -1243.27\n",
       " -1719.06    -1990.46    -2263.85\n",
       "  2744.65     2938.57     1888.78\n",
       "  -713.216    -989.031    -577.639\n",
       "  -632.527    -260.29     -136.677\n",
       "   -35.3259   -770.531    -211.496\n",
       "   614.455    -106.467     226.903\n",
       "     ⋮                   \n",
       "  1020.54     1181.68     -834.841\n",
       "   865.607    1177.11     1517.47\n",
       "  1917.12     2010.56     1873.69\n",
       " -2637.12    -2694.13    -2880.99\n",
       "  -932.128    -812.192   -1166.91\n",
       "   -78.4848   -139.635     -67.6904\n",
       " -2350.67    -2271.85    -1545.87\n",
       "   452.867     716.946    -428.45\n",
       "  -927.311    -841.576    -782.454\n",
       "\n",
       "[:, :, 283] =\n",
       " -1739.63    -1485.49    -1429.11\n",
       "  1830.54     1784.83      757.405\n",
       "   -38.3072    -26.6851  -1272.61\n",
       "  -355.33     -510.166   -1225.39\n",
       " -1820.79    -2101.28    -2293.18\n",
       "  2838.77     2985.34     1949.94\n",
       "  -873.292   -1029.56     -568.509\n",
       "  -454.822    -151.209    -143.301\n",
       "  -415.073    -945.695    -150.042\n",
       "   267.202    -308.398     260.457\n",
       "     ⋮                   \n",
       "  1161.32     1153.31     -820.75\n",
       "  1034.68     1235.34     1458.98\n",
       "  1954.96     2043.04     1902.01\n",
       " -2701.04    -2664.95    -2796.59\n",
       "  -876.293    -776.871   -1161.34\n",
       "   -97.1732   -167.064     -77.2828\n",
       " -2300.16    -2267.03    -1624.64\n",
       "   622.038     740.482    -488.244\n",
       "  -863.166    -844.978    -828.533\n",
       "\n",
       "[:, :, 284] =\n",
       " -1579.82    -1605.61    -1467.58\n",
       "  1818.07     1823.09      761.548\n",
       "   -27.9464    -28.586   -1245.03\n",
       "  -452.614    -436.703   -1200.6\n",
       " -1981.43    -1951.14    -2311.58\n",
       "  2932.46     2916.85     2006.54\n",
       "  -986.172    -971.715    -557.302\n",
       "  -265.827    -296.299    -155.307\n",
       "  -760.173    -708.182     -90.4134\n",
       "   -93.238     -35.4519    293.751\n",
       "     ⋮                   \n",
       "  1184.15     1187.48     -802.23\n",
       "  1172.1      1152.46     1402.29\n",
       "  2008.77     1999.58     1926.92\n",
       " -2693.96    -2698.1     -2717.32\n",
       "  -814.79     -824.846   -1153.59\n",
       "  -136.681    -128.755     -87.0802\n",
       " -2272.02    -2274.88    -1697.88\n",
       "   713.912     703.101    -536.684\n",
       "  -840.436    -841.235    -872.137"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_coordinates = rearrange(locations[1], (:K, 1, :L) --> (:K, :L))\n",
    "rotated_coordinates = transform_molecule(input_coordinates)\n",
    "coordinate_tokens = rearrange(rotated_coordinates, (:K, :L) --> (1, :K, :L))\n",
    "clock_tokens = rff(coordinate_tokens)\n",
    "embeddings = pre_transformer_proj(clock_tokens)\n",
    "for block in transformer_blocks\n",
    "    embeddings = transformer_block(embeddings)\n",
    "end\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37071bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Onion.causal_mask(rand(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b550b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###foot\n",
    "\n",
    "# Parameters for the MOGMOG model\n",
    "const rff_dim = 32               # Random Fourier feature dimension\n",
    "const embed_dim = 64             # Final transformer input dim\n",
    "const vocab_size = 10            # Number of atom types (incl. STOP)\n",
    "\n",
    "# -- Modules for the foot --\n",
    "const RFF = RandomFourierFeatures(3 => rff_dim, 0.1f0)  # 3D input\n",
    "const CoordProj = Dense(rff_dim => embed_dim, bias=false)\n",
    "const AtomEmbed = Embedding(vocab_size, embed_dim)\n",
    "###################################################################################3\n",
    "\n",
    "struct CoordEmbed \n",
    "    mlp::Chain #this is a multi-layer perceptron (MLP)\n",
    "end\n",
    "\n",
    "function CoordEmbed(embed_dim::Int)\n",
    "    return CoordEmbed(Chain(\n",
    "        Dense(3, embed_dim, relu),\n",
    "        Dense(embed_dim, embed_dim)\n",
    "    ))\n",
    "end\n",
    "\n",
    "function (ce::CoordEmbed)(coords::Matrix{Float64})  # (3, L)\n",
    "    return ce.mlp(coords')'  # Transpose for batch → return (embed_dim, L)\n",
    "end \n",
    "###################################################################################3\n",
    "\n",
    "function encode_prefix(positions::Matrix{Float64}, atom_types::Vector{Int}, ce::CoordEmbed, ae::Embedding)\n",
    "    # positions: (3, L)\n",
    "    # atom_types: (L,)\n",
    "\n",
    "    coord_embed = ce(positions)         # (embed_dim, L)\n",
    "    type_embed = ae(atom_types)         # (embed_dim, L)\n",
    "\n",
    "    return coord_embed .+ type_embed    # Combine the two\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b06dbad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/MOGMOG.jl-1`\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "SystemError: opening file \"/home/star/MOGMOG.jl-1/Notebook/loss.jl\": No such file or directory",
     "output_type": "error",
     "traceback": [
      "SystemError: opening file \"/home/star/MOGMOG.jl-1/Notebook/loss.jl\": No such file or directory\n",
      "\n",
      "Stacktrace:\n",
      " [1] include(fname::String)\n",
      "   @ Main ./sysimg.jl:38\n",
      " [2] top-level scope\n",
      "   @ ~/MOGMOG.jl-1/Notebook/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X21sZmlsZQ==.jl:6"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"..\")\n",
    "\n",
    "using Flux, JLD2, Random, Statistics, MOGMOG, Onion, RandomFeatureMaps, Onion.Einops\n",
    "\n",
    "include(\"loss.jl\")\n",
    "include(\"MOGhead.jl\")\n",
    "include(\"MOGfoot.jl\")\n",
    "include(\"model.jl\")\n",
    "include(\"utilities.jl\")\n",
    "\n",
    "export logpdf_MOG, MoGAxisHead, MOGMOGModel, transform_molecule, loss\n",
    "\n",
    "# Load molecules\n",
    "@load \"processed_molecules.jld2\" result\n",
    "\n",
    "# Atom dictionary\n",
    "atom_dict = Dict(name => i for (i, name) in enumerate([\"C\", \"F\", \"H\", \"N\", \"O\", \"STOP\"]))\n",
    "PAD = atom_dict[\"STOP\"]\n",
    "\n",
    "# Hyperparameters\n",
    "embed_dim = 64\n",
    "n_components = 5\n",
    "vocab_size = length(atom_dict)\n",
    "depth = 4\n",
    "max_len = 32\n",
    "batchsize = 4\n",
    "nbatches = 1000\n",
    "nepochs = 100\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = MOGMOGModel(embed_dim, n_components, vocab_size, depth=depth)\n",
    "opt_state = Flux.setup(AdamW(0.001f0), model)\n",
    "\n",
    "# Padding function\n",
    "function pad_batch(mols::Vector{Molecule})\n",
    "    B = length(mols)\n",
    "    atom_ids = fill(PAD, max_len, B)\n",
    "    positions = zeros(Float32, 3, max_len, B)\n",
    "    atom_mask = zeros(Float32, max_len - 1, B)\n",
    "    coord_mask = zeros(Float32, max_len - 1, B)\n",
    "\n",
    "    for (i, mol) in enumerate(mols)\n",
    "        L = min(length(mol.atoms), max_len - 1)\n",
    "        for j in 1:L\n",
    "            atom_ids[j, i] = atom_dict[mol.atoms[j]]\n",
    "            positions[:, j, i] = Float32.(mol.positions[j, :])\n",
    "            atom_mask[j, i] = 1.0\n",
    "            coord_mask[j, i] = 1.0\n",
    "        end\n",
    "        atom_ids[L+1, i] = PAD\n",
    "    end\n",
    "    return atom_ids, positions, atom_mask, coord_mask\n",
    "end\n",
    "\n",
    "# Training loop\n",
    "all_losses = Float32[]\n",
    "for epoch in 1:nepochs\n",
    "    println(\"Epoch $epoch\")\n",
    "    shuffled = shuffle(result)\n",
    "\n",
    "    for i in 1:nbatches\n",
    "        mols = shuffled[i:i+batchsize-1]\n",
    "        atom_ids, pos, atom_mask, coord_mask = pad_batch(mols)\n",
    "\n",
    "        loss_val, (grad,) = Flux.withgradient(model) do m\n",
    "            loss(m, atom_ids, pos, atom_mask, coord_mask)\n",
    "        end\n",
    "\n",
    "        Flux.update!(opt_state, model, grad)\n",
    "        push!(all_losses, loss_val)\n",
    "\n",
    "        println(\"Batch $i, loss = $(round(loss_val, digits=4))\")\n",
    "    end\n",
    "end\n",
    "\n",
    "# Save loss plot\n",
    "using Plots\n",
    "plot(all_losses, title=\"Training Loss\", xlabel=\"Batch\", ylabel=\"Loss\")\n",
    "savefig(\"training_loss.pdf\")\n",
    "\n",
    "# Save checkpoint\n",
    "@save \"mogmodel_checkpoint.jld2\" model all_losses\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
